{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyt/miniconda3/envs/DocRED/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel,RobertaTokenizer\n",
    "from utils import *\n",
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from kge.model import KgeModel\n",
    "from kge.util.io import load_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration of dataset fbwq_half...\n",
      "Setting complex.relation_embedder.dropout to 0, was set to -0.4746062345802784.\n",
      "Loading half entity_ids.del\n"
     ]
    }
   ],
   "source": [
    "kg_type = 'half'\n",
    "hops = 'webqsp_half'\n",
    "checkpoint_file = '../../pretrained_models/embeddings/ComplEx_fbwq_' + kg_type + '/checkpoint_best.pt'\n",
    "kge_checkpoint = load_checkpoint(checkpoint_file)\n",
    "kge_model = KgeModel.create_from(kge_checkpoint)\n",
    "kge_model.eval()\n",
    "e = getEntityEmbeddings(kge_model, hops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataloader import DatasetMetaQA, DataLoaderMetaQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file processed, making dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /data/lyt/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /data/lyt/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/QA_data/WebQuestionsSP/qa_train_webqsp.txt'\n",
    "entity2idx, idx2entity, embedding_matrix = prepare_embeddings(e)\n",
    "data = process_text_file(data_path, split=False)\n",
    "print('Train file processed, making dataloader')\n",
    "# word2ix,idx2word, max_len = get_vocab(data)\n",
    "# hops = str(num_hops)\n",
    "device = torch.device(device)\n",
    "dataset = DatasetMetaQA(data, e, entity2idx)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokenized = a[0].to(device)\n",
    "attention_mask = a[1].to(device)\n",
    "positive_head = a[2].to(device)\n",
    "positive_tail = a[3].to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' what is the name of justin bieber brother NE '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.decode(question_tokenized[0].tolist(),skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>There is a test</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = roberta_tokenizer.encode('There is a test')\n",
    "roberta_tokenizer.decode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    99,    16,     5,   766,     9,    95,   179,   741,   324,\n",
       "          1943,  2138, 12462,  1437,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]], device='cuda:7')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0658,  0.0565, -0.0207,  ..., -0.1221, -0.0829,  0.0122],\n",
       "         [ 0.0497, -0.5093,  0.1265,  ..., -0.0039,  0.1520,  0.0716],\n",
       "         [ 0.3459,  0.0347,  0.0264,  ..., -0.2519,  0.2735,  0.1994],\n",
       "         ...,\n",
       "         [ 0.0700, -0.0940,  0.1141,  ..., -0.0928, -0.1418,  0.1312],\n",
       "         [ 0.0700, -0.0940,  0.1141,  ..., -0.0928, -0.1418,  0.1312],\n",
       "         [ 0.0700, -0.0940,  0.1141,  ..., -0.0928, -0.1418,  0.1312]]],\n",
       "       device='cuda:7', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 2.1804e-03, -2.1176e-01, -2.1001e-01, -6.5291e-02,  1.3897e-01,\n",
       "          2.1853e-01,  2.6736e-01, -7.9871e-02, -8.8498e-02, -1.5445e-01,\n",
       "          2.2400e-01, -2.3583e-02, -9.8725e-02,  9.0742e-02, -1.3481e-01,\n",
       "          4.9209e-01,  2.2070e-01, -4.5514e-01,  3.2645e-02, -2.6803e-02,\n",
       "         -2.6233e-01,  4.7910e-02,  4.6291e-01,  3.2811e-01,  1.0480e-01,\n",
       "          6.7569e-02, -1.4618e-01, -4.8004e-02,  1.7808e-01,  2.1060e-01,\n",
       "          2.9447e-01,  5.2326e-02,  1.1039e-01,  2.4770e-01, -2.3891e-01,\n",
       "          5.6179e-02, -3.1096e-01,  3.1632e-02,  2.6038e-01, -1.9329e-01,\n",
       "         -7.3438e-02,  1.5077e-01,  2.2308e-01, -1.2913e-01, -9.1351e-02,\n",
       "          4.0307e-01,  2.4430e-01,  2.0971e-02, -1.3702e-01, -9.5093e-02,\n",
       "         -3.6891e-01,  3.5633e-01,  2.7938e-01,  1.9806e-01, -2.7992e-02,\n",
       "          4.3784e-02, -1.5972e-01,  2.3459e-01, -9.6601e-02, -1.1347e-01,\n",
       "         -1.2023e-01, -1.9306e-01, -2.0825e-02, -5.1383e-02,  2.3044e-02,\n",
       "         -1.4567e-01,  8.4692e-02, -1.3430e-01, -1.3167e-01,  6.8259e-02,\n",
       "         -9.6914e-02,  1.1379e-01,  1.6042e-01, -2.9496e-01, -2.8365e-01,\n",
       "          4.7260e-02, -5.8808e-01, -1.2293e-01,  2.9463e-01,  4.3252e-01,\n",
       "         -1.1035e-01,  1.7250e-01,  3.7626e-02,  2.1649e-01, -8.9176e-03,\n",
       "         -7.8046e-02, -3.8040e-02, -9.9288e-02,  1.9971e-01,  2.7903e-01,\n",
       "         -2.0475e-01, -3.7374e-01,  4.8553e-02,  1.7312e-02, -9.2162e-02,\n",
       "          1.8921e-02, -3.6160e-02, -7.4109e-02, -1.7034e-01, -1.7516e-01,\n",
       "          7.0253e-02, -2.7109e-01, -1.4047e-01,  2.5428e-01, -3.1533e-02,\n",
       "         -1.9956e-01, -4.6908e-03,  2.8411e-01,  7.3670e-02, -1.0883e-01,\n",
       "         -1.6101e-01,  4.3590e-01,  3.1708e-01,  9.9980e-03, -9.7300e-03,\n",
       "          1.8956e-01,  1.3178e-01, -2.8258e-01,  4.3456e-01, -3.1258e-01,\n",
       "         -1.4691e-02, -1.1724e-01,  9.9369e-02,  1.5912e-01, -2.1255e-01,\n",
       "          2.7608e-01,  1.2799e-01,  2.7319e-01,  1.8675e-01,  7.6582e-02,\n",
       "         -2.2899e-02,  1.1937e-01, -1.2541e-01,  1.5760e-01,  2.1906e-01,\n",
       "          1.2690e-01,  5.7559e-03, -3.2411e-01, -2.1020e-01,  2.7529e-01,\n",
       "          3.2714e-01,  1.6201e-01, -3.0887e-02,  1.8395e-01,  1.0558e-01,\n",
       "          2.2976e-01,  1.5037e-01, -4.2491e-01,  3.1116e-02,  3.4922e-01,\n",
       "          9.1724e-02,  1.7203e-01, -9.8659e-02, -2.7632e-01, -2.5988e-01,\n",
       "         -8.2932e-02,  3.3370e-02, -3.2340e-01, -1.0949e-01,  3.4837e-01,\n",
       "          2.7402e-02,  2.3040e-02, -1.4284e-01, -2.5475e-01, -4.5611e-02,\n",
       "         -1.1037e-01,  2.3412e-02,  9.9397e-02, -7.3450e-02, -3.9633e-01,\n",
       "         -8.7209e-02, -5.4961e-01, -1.2243e-01,  1.7585e-01, -3.1477e-01,\n",
       "          2.3937e-01, -3.0364e-01,  9.4181e-02,  3.9135e-01,  1.6568e-02,\n",
       "         -1.2621e-03, -1.6095e-01, -1.9138e-02,  9.7311e-02,  3.0025e-01,\n",
       "          2.4401e-01, -3.8602e-01,  1.0961e-01,  1.5262e-01,  2.5662e-01,\n",
       "          1.5839e-01, -2.2652e-02, -1.3017e-01,  1.2825e-01, -1.9134e-01,\n",
       "          1.7925e-01, -2.2609e-01,  1.8960e-01, -2.3962e-01, -2.2364e-01,\n",
       "          2.8858e-01, -4.0879e-01, -4.4097e-02,  9.0923e-02,  2.6922e-01,\n",
       "          2.1789e-02, -4.8868e-02, -8.8431e-02,  1.2200e-01,  1.6296e-01,\n",
       "          1.4687e-01, -3.9226e-01,  2.6959e-01, -5.4300e-02, -2.0774e-02,\n",
       "         -2.4618e-02,  1.7021e-01,  2.3614e-01,  6.9768e-02, -3.9868e-01,\n",
       "         -1.5063e-01,  1.0207e-01,  2.8127e-01, -2.2116e-01,  1.8716e-01,\n",
       "         -2.7856e-01, -3.9601e-01, -1.3146e-01,  2.1034e-01,  2.2668e-01,\n",
       "          1.4421e-01, -2.7630e-01,  1.8994e-01, -9.7969e-02, -4.1951e-01,\n",
       "         -3.6473e-01, -1.0058e-01,  2.2705e-01,  1.9701e-01,  2.0560e-01,\n",
       "          2.3705e-01,  2.4325e-02,  1.2168e-01,  1.3212e-01,  1.5245e-01,\n",
       "         -1.3819e-01,  1.9264e-01, -3.4639e-01, -4.9136e-02, -2.6290e-01,\n",
       "         -1.8752e-01, -2.2049e-01,  3.7650e-01, -2.2550e-01,  2.3012e-01,\n",
       "          3.9003e-01, -2.8705e-01, -9.5436e-02,  1.5233e-01,  8.9342e-02,\n",
       "          7.0839e-02, -1.1955e-01,  2.0314e-01,  1.6852e-01, -1.2195e-01,\n",
       "          2.3137e-01, -1.7420e-03,  2.6980e-01,  1.8891e-01,  1.2190e-01,\n",
       "          1.4096e-01,  1.1659e-01, -1.5404e-01,  6.5904e-02,  1.1218e-02,\n",
       "         -1.8593e-02, -2.3885e-01, -1.5380e-01,  2.1012e-01, -4.9626e-02,\n",
       "          2.7275e-02, -1.7572e-01, -1.0198e-01,  2.6049e-02,  4.0020e-01,\n",
       "         -3.6617e-01,  2.6573e-01,  5.7316e-02,  1.6930e-01, -2.1916e-01,\n",
       "         -2.0198e-01,  8.4335e-02,  1.7616e-01, -3.9744e-01, -1.0257e-04,\n",
       "          1.5093e-01,  1.1384e-01,  2.0403e-01,  2.6422e-01, -3.1008e-03,\n",
       "         -8.1315e-02,  5.0013e-01, -1.3256e-01, -1.3196e-01,  2.6885e-01,\n",
       "         -2.4436e-01, -2.8940e-01,  2.4568e-01, -2.3323e-02,  3.1594e-01,\n",
       "          1.3664e-01,  2.9568e-02,  6.9995e-02, -6.0508e-01,  8.9428e-02,\n",
       "         -4.5931e-01,  5.0440e-03,  2.4904e-02, -8.4134e-02, -2.0891e-01,\n",
       "          1.6019e-01,  3.0307e-01, -2.5166e-01, -4.4590e-02,  1.8652e-01,\n",
       "          8.4504e-02, -1.3026e-01,  4.8366e-01, -2.0850e-02,  2.2323e-01,\n",
       "         -7.9914e-02,  2.5803e-01, -1.9151e-01,  2.6805e-01, -2.7492e-01,\n",
       "         -7.9751e-02,  5.4839e-03,  7.7908e-02,  6.0504e-02, -4.6951e-02,\n",
       "         -3.1991e-01,  2.1200e-01, -2.5991e-02, -4.9959e-02, -5.2809e-02,\n",
       "          9.2735e-02,  2.1724e-03,  5.0874e-02,  4.2747e-02,  3.2074e-01,\n",
       "          2.1321e-01, -3.3313e-02, -3.7611e-01, -2.8039e-02, -9.9344e-02,\n",
       "          4.9864e-02,  2.3146e-02, -1.0948e-02,  4.2724e-01, -1.0889e-01,\n",
       "          1.8055e-02, -1.3953e-01,  2.5832e-01,  2.2559e-01,  1.3625e-01,\n",
       "          1.2403e-01,  6.4547e-02,  1.5044e-01, -5.5205e-02, -1.4676e-02,\n",
       "         -1.4272e-01, -2.3693e-01, -2.8319e-01,  2.1133e-01, -2.2347e-01,\n",
       "         -1.7096e-01,  1.6526e-01,  2.2033e-01, -1.2816e-01,  1.2296e-01,\n",
       "          3.0457e-01,  1.0631e-01, -1.4988e-01,  2.7032e-01, -1.2197e-01,\n",
       "          1.0284e-01,  2.9655e-01, -2.0821e-02,  1.7980e-01,  5.0549e-01,\n",
       "          2.1192e-01, -3.6025e-01, -2.0586e-02, -2.3794e-01,  3.3199e-03,\n",
       "          2.5660e-01, -1.5049e-01,  1.8358e-01,  3.8240e-01,  3.1875e-01,\n",
       "          4.5937e-01, -1.3724e-02, -1.2963e-01,  1.0829e-01,  2.0329e-01,\n",
       "          2.2275e-02, -1.3924e-01, -1.5340e-01,  2.6638e-01,  3.1963e-02,\n",
       "         -1.5898e-01, -3.1976e-02, -1.0667e-01,  4.9624e-02, -1.2733e-01,\n",
       "         -3.9011e-01,  4.3131e-02,  2.1793e-01, -4.7300e-01,  9.2354e-02,\n",
       "         -2.8528e-01,  3.6638e-02, -2.2776e-01,  2.1895e-01, -2.1984e-01,\n",
       "         -1.2561e-01,  3.9368e-01, -1.0233e-01,  4.3314e-02, -1.7455e-01,\n",
       "         -1.2829e-01,  2.9724e-02,  4.6060e-03, -2.7220e-02, -2.7161e-02,\n",
       "          3.6911e-01, -1.3985e-01,  1.1412e-02,  1.8220e-02,  2.1094e-01,\n",
       "         -4.8352e-02,  1.8047e-01,  3.4201e-02, -1.3274e-01, -3.6489e-01,\n",
       "          1.6624e-01, -2.0038e-01, -4.2796e-01, -3.5996e-01,  3.3903e-01,\n",
       "         -1.3395e-01, -2.6169e-01, -2.2028e-01, -2.6503e-01,  6.3813e-02,\n",
       "          1.7608e-01,  4.5948e-01, -3.7608e-01, -6.8321e-02,  4.9225e-01,\n",
       "         -5.5985e-02, -1.7207e-01,  3.1046e-01,  2.1480e-01, -3.0710e-01,\n",
       "          3.4175e-01,  2.7833e-01, -5.3270e-02,  2.1835e-02,  5.1408e-01,\n",
       "          1.4001e-01,  1.9611e-01, -2.2050e-01,  4.5132e-01, -2.0976e-01,\n",
       "          3.0752e-01, -1.4414e-01, -2.0003e-01, -1.9775e-01, -7.0510e-03,\n",
       "          3.2473e-01,  1.7795e-01, -4.1102e-01, -1.1316e-01,  3.3272e-02,\n",
       "          3.4554e-01, -4.0111e-01, -8.8683e-02,  2.6612e-03, -3.1752e-01,\n",
       "          1.2968e-01,  9.3314e-02,  2.2348e-01, -3.7963e-01, -2.2115e-03,\n",
       "          4.0338e-01, -3.2502e-01,  1.1528e-01,  2.9771e-01,  8.9318e-02,\n",
       "          3.5124e-01, -2.4309e-02, -2.8485e-03,  5.2722e-02, -2.2492e-01,\n",
       "         -3.7062e-02,  1.6183e-01,  5.4819e-01,  1.5371e-01, -3.7583e-01,\n",
       "          1.1500e-01,  2.3503e-01, -1.5572e-01,  2.9974e-01, -9.2552e-02,\n",
       "         -3.8941e-02,  2.5990e-01, -3.6237e-02,  1.2037e-01, -1.0005e-01,\n",
       "         -2.3657e-01, -3.0246e-01,  3.4897e-01, -1.9483e-01, -1.2027e-01,\n",
       "         -1.6414e-01, -1.0240e-01, -1.3846e-01,  5.8365e-02, -3.7144e-01,\n",
       "          3.3979e-01,  1.2389e-01, -1.9558e-01, -1.0358e-01, -7.7914e-02,\n",
       "         -1.5296e-01, -2.1930e-01, -2.3583e-01,  4.2622e-01, -1.6847e-01,\n",
       "         -4.4717e-01,  2.5944e-01,  4.2209e-02,  3.5123e-01,  4.2388e-02,\n",
       "          8.6343e-02, -5.3434e-02,  1.3974e-01,  9.6878e-02, -1.2834e-01,\n",
       "          2.7580e-01,  6.7755e-02, -5.5872e-01, -1.4413e-01, -2.2402e-01,\n",
       "          9.0878e-02,  1.7276e-01, -3.4960e-01,  1.1194e-02,  4.0715e-02,\n",
       "          1.2839e-01,  2.8841e-02, -1.1455e-01, -6.8902e-02,  4.0175e-01,\n",
       "          2.3143e-01,  2.8139e-01,  8.1478e-02,  2.3180e-01,  9.3832e-04,\n",
       "         -3.1851e-01,  4.8216e-02,  8.4555e-02, -1.8492e-01,  4.4976e-01,\n",
       "         -9.8012e-02, -4.1322e-01, -7.9498e-02,  4.0554e-01,  1.1831e-01,\n",
       "         -6.8833e-03, -3.6234e-02,  2.1621e-01,  1.6414e-01, -1.2905e-01,\n",
       "          1.7721e-01, -2.1541e-02, -1.3315e-01, -1.0102e-01,  9.9729e-02,\n",
       "         -2.2400e-01,  5.1874e-02, -1.6485e-01, -1.8338e-02, -2.0211e-01,\n",
       "          5.5530e-03, -2.0025e-01,  2.3791e-01, -3.2711e-01,  1.1898e-01,\n",
       "          6.6129e-02,  3.0560e-01, -3.3606e-01, -1.7155e-01, -5.2834e-02,\n",
       "          1.6204e-01,  2.7342e-01,  3.4113e-01,  3.1593e-02,  1.4489e-02,\n",
       "         -1.7616e-01, -2.6180e-01,  8.2986e-02, -1.9681e-01,  1.5025e-01,\n",
       "          7.2368e-02,  2.3679e-01, -3.1917e-01, -1.9747e-01,  2.1816e-01,\n",
       "         -9.1823e-02, -1.2716e-01,  4.0580e-01,  2.2272e-01,  2.0924e-01,\n",
       "          1.9185e-02,  2.5706e-01,  4.0171e-02, -2.0810e-01, -1.3696e-01,\n",
       "         -2.5924e-01,  8.3753e-02, -9.6358e-02, -6.2164e-02, -7.5774e-02,\n",
       "         -1.5305e-01, -1.9094e-01, -1.4431e-01,  1.4420e-01,  1.1966e-01,\n",
       "          2.7730e-02, -5.0901e-02, -3.5553e-02, -2.7749e-01,  2.9876e-01,\n",
       "          1.1316e-02,  6.2008e-02, -6.8343e-02,  4.4862e-02, -1.5199e-01,\n",
       "          2.4588e-01,  2.1812e-01,  8.4910e-02, -1.9487e-01, -8.6816e-02,\n",
       "         -2.7625e-01, -3.4912e-01,  7.0303e-02,  1.3315e-01,  1.2356e-01,\n",
       "         -1.0590e-01, -2.8288e-01, -8.3726e-03, -1.3345e-01,  1.8414e-01,\n",
       "          1.3140e-02, -1.5889e-01, -7.6535e-02, -6.0267e-02, -4.1152e-02,\n",
       "          8.0766e-02, -2.1742e-01, -1.8993e-01, -1.1624e-01, -7.6687e-02,\n",
       "         -7.3879e-02,  3.4818e-01, -5.8565e-02,  2.8858e-01, -1.2909e-01,\n",
       "          1.5334e-02, -1.6437e-01,  1.1733e-01, -4.5973e-02,  8.2259e-02,\n",
       "          2.7176e-01, -4.6946e-01, -1.6280e-01, -7.7368e-03, -2.0664e-01,\n",
       "         -1.5213e-01, -8.5305e-02, -2.9794e-02,  2.1053e-01, -3.5307e-01,\n",
       "          2.1785e-01, -1.0446e-01,  1.6904e-01, -7.0661e-02, -2.7357e-01,\n",
       "         -1.6497e-01,  3.4847e-03,  2.5556e-01, -3.4632e-01, -2.4251e-01,\n",
       "         -2.6276e-01, -1.0143e-01, -8.7722e-02, -2.6037e-01,  4.1172e-01,\n",
       "         -1.3581e-01, -8.8289e-02,  2.7686e-02,  4.5113e-01,  1.9371e-01,\n",
       "          1.7672e-01,  2.0805e-01, -7.0580e-03,  3.9003e-02,  1.2423e-01,\n",
       "         -4.9065e-01,  2.2979e-01, -2.4576e-01, -1.2257e-01, -3.2074e-03,\n",
       "          9.6117e-02, -8.9728e-03,  3.2376e-02, -1.5444e-01, -9.9592e-02,\n",
       "          2.0777e-01, -3.6746e-01, -4.4051e-02,  2.7172e-01,  1.6920e-01,\n",
       "         -2.5739e-01,  7.8771e-03,  1.2970e-01,  3.8032e-01,  9.0042e-02,\n",
       "         -2.3063e-01,  1.2072e-01, -3.4612e-01, -5.0331e-02, -2.1143e-01,\n",
       "         -2.8609e-01,  1.5655e-01, -7.6566e-02,  9.0595e-02, -1.0131e-01,\n",
       "         -2.8440e-01,  2.0549e-01, -6.3989e-02, -7.4141e-02,  4.2234e-01,\n",
       "          2.0512e-02, -1.3363e-01,  1.2270e-01, -3.4240e-03,  1.9289e-02,\n",
       "         -1.1591e-01,  2.7572e-01,  1.9894e-01, -2.8248e-01,  1.5535e-01,\n",
       "         -1.2330e-01, -4.8072e-02, -8.9300e-02]], device='cuda:7',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model(question_tokenized, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DocRED')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a122494871ff66f22689dd27411b6f9b224429266c3689c062961e2d5178a3e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
